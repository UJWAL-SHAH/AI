{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J078_Learning_Module_Nim_Problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqIfFhgEqX7xLhBo02tyQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UJWAL-SHAH/AI/blob/master/edx_Project/Learning_Module/J078_Learning_Module_Nim_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5YWg1McvwwG"
      },
      "source": [
        "#Roll No: J078\n",
        "\n",
        "#Learning Module - Nim Problem\n",
        "\n",
        "Background\n",
        "\n",
        "Recall that in the game Nim, we begin with some number of piles, each with some number of objects. Players take turns: on a player’s turn, the player removes any non-negative number of objects from any one non-empty pile. Whoever removes the last object loses.\n",
        "\n",
        "There’s some simple strategy you might imagine for this game: if there’s only one pile and three objects left in it, and it’s your turn, your best bet is to remove two of those objects, leaving your opponent with the third and final object to remove. But if there are more piles, the strategy gets considerably more complicated. In this problem, we’ll build an AI to learn the strategy for this game through reinforcement learning. By playing against itself repeatedly and learning from experience, eventually our AI will learn which actions to take and which actions to avoid.\n",
        "\n",
        "In particular, we’ll use Q-learning for this project. Recall that in Q-learning, we try to learn a reward value (a number) for every (state, action) pair. An action that loses the game will have a reward of -1, an action that results in the other player losing the game will have a reward of 1, and an action that results in the game continuing has an immediate reward of 0, but will also have some future reward.\n",
        "\n",
        "How will we represent the states and actions inside of a Python program? A “state” of the Nim game is just the current size of all of the piles. A state, for example, might be [1, 1, 3, 5], representing the state with 1 object in pile 0, 1 object in pile 1, 3 objects in pile 2, and 5 objects in pile 3. An “action” in the Nim game will be a pair of integers (i, j), representing the action of taking j objects from pile i. So the action (3, 5) represents the action “from pile 3, take away 5 objects.” Applying that action to the state [1, 1, 3, 5] would result in the new state [1, 1, 3, 0] (the same state, but with pile 3 now empty).\n",
        "\n",
        "Recall that the key formula for Q-learning is below. Every time we are in a state s and take an action a, we can update the Q-value Q(s, a) according to:\n",
        "\n",
        "Q(s, a) <- Q(s, a) + alpha * (new value estimate - old value estimate)\n",
        "\n",
        "\n",
        "In the above formula, alpha is the learning rate (how much we value new information compared to information we already have). The new value estimate represents the sum of the reward received for the current action and the estimate of all the future rewards that the player will receive. The old value estimate is just the existing value for Q(s, a). By applying this formula every time our AI takes a new action, over time our AI will start to learn which actions are better in any state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCeE_YZJvpfv"
      },
      "source": [
        "#Importing libraries\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "#Defining Class Nim\n",
        "class Nim():\n",
        "\n",
        "    def __init__(self, initial=[1, 3, 5, 7]):\n",
        "        \"\"\"\n",
        "        Initialize game board.\n",
        "        Each game board has\n",
        "            - `piles`: a list of how many elements remain in each pile\n",
        "            - `player`: 0 or 1 to indicate which player's turn\n",
        "            - `winner`: None, 0, or 1 to indicate who the winner is\n",
        "        \"\"\"\n",
        "        self.piles = initial.copy()\n",
        "        self.player = 0\n",
        "        self.winner = None\n",
        "\n",
        "    @classmethod\n",
        "    def available_actions(cls, piles):\n",
        "        \"\"\"\n",
        "        Nim.available_actions(piles) takes a `piles` list as input\n",
        "        and returns all of the available actions `(i, j)` in that state.\n",
        "        Action `(i, j)` represents the action of removing `j` items\n",
        "        from pile `i` (where piles are 0-indexed).\n",
        "        \"\"\"\n",
        "        actions = set()\n",
        "        for i, pile in enumerate(piles):\n",
        "            for j in range(1, pile + 1):\n",
        "                actions.add((i, j))\n",
        "        return actions\n",
        "\n",
        "    @classmethod\n",
        "    def other_player(cls, player):\n",
        "        \"\"\"\n",
        "        Nim.other_player(player) returns the player that is not\n",
        "        `player`. Assumes `player` is either 0 or 1.\n",
        "        \"\"\"\n",
        "        return 0 if player == 1 else 1\n",
        "\n",
        "    def switch_player(self):\n",
        "        \"\"\"\n",
        "        Switch the current player to the other player.\n",
        "        \"\"\"\n",
        "        self.player = Nim.other_player(self.player)\n",
        "\n",
        "    def move(self, action):\n",
        "        \"\"\"\n",
        "        Make the move `action` for the current player.\n",
        "        `action` must be a tuple `(i, j)`.\n",
        "        \"\"\"\n",
        "        pile, count = action\n",
        "\n",
        "        # Check for errors\n",
        "        if self.winner is not None:\n",
        "            raise Exception(\"Game already won\")\n",
        "        elif pile < 0 or pile >= len(self.piles):\n",
        "            raise Exception(\"Invalid pile\")\n",
        "        elif count < 1 or count > self.piles[pile]:\n",
        "            raise Exception(\"Invalid number of objects\")\n",
        "\n",
        "        # Update pile\n",
        "        self.piles[pile] -= count\n",
        "        self.switch_player()\n",
        "\n",
        "        # Check for a winner\n",
        "        if all(pile == 0 for pile in self.piles):\n",
        "            self.winner = self.player\n",
        "\n",
        "\n",
        "class NimAI():\n",
        "\n",
        "    def __init__(self, alpha=0.5, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize AI with an empty Q-learning dictionary,\n",
        "        an alpha (learning) rate, and an epsilon rate.\n",
        "        The Q-learning dictionary maps `(state, action)`\n",
        "        pairs to a Q-value (a number).\n",
        "         - `state` is a tuple of remaining piles, e.g. (1, 1, 4, 4)\n",
        "         - `action` is a tuple `(i, j)` for an action\n",
        "        \"\"\"\n",
        "        self.q = dict()\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def update(self, old_state, action, new_state, reward):\n",
        "        \"\"\"\n",
        "        Update Q-learning model, given an old state, an action taken\n",
        "        in that state, a new resulting state, and the reward received\n",
        "        from taking that action.\n",
        "        \"\"\"\n",
        "        old = self.get_q_value(old_state, action)\n",
        "        best_future = self.best_future_reward(new_state)\n",
        "        self.update_q_value(old_state, action, old, reward, best_future)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        \"\"\"\n",
        "        Return the Q-value for the state `state` and the action `action`.\n",
        "        If no Q-value exists yet in `self.q`, return 0.\n",
        "        \"\"\"\n",
        "        if (tuple(state), action) in self.q:\n",
        "            return self.q[(tuple(state), action)]\n",
        "        else: \n",
        "            return 0\n",
        "\n",
        "    def update_q_value(self, state, action, old_q, reward, future_rewards):\n",
        "        \"\"\"\n",
        "        Update the Q-value for the state `state` and the action `action`\n",
        "        given the previous Q-value `old_q`, a current reward `reward`,\n",
        "        and an estiamte of future rewards `future_rewards`.\n",
        "        Use the formula:\n",
        "        Q(s, a) <- old value estimate\n",
        "                   + alpha * (new value estimate - old value estimate)\n",
        "        where `old value estimate` is the previous Q-value,\n",
        "        `alpha` is the learning rate, and `new value estimate`\n",
        "        is the sum of the current reward and estimated future rewards.\n",
        "        \"\"\"\n",
        "        self.q[(tuple(state), action)] = old_q + self.alpha * (future_rewards + reward - old_q)\n",
        "\n",
        "    def best_future_reward(self, state):\n",
        "        \"\"\"\n",
        "        Given a state `state`, consider all possible `(state, action)`\n",
        "        pairs available in that state and return the maximum of all\n",
        "        of their Q-values.\n",
        "        Use 0 as the Q-value if a `(state, action)` pair has no\n",
        "        Q-value in `self.q`. If there are no available actions in\n",
        "        `state`, return 0.\n",
        "        \"\"\"\n",
        "        bestOutcome = 0\n",
        "        optionsAvail = list(Nim.available_actions(state))\n",
        "        for i in optionsAvail:\n",
        "            bestOutcome = max(self.get_q_value(state, i), bestOutcome)\n",
        "        \n",
        "        return bestOutcome\n",
        "\n",
        "    def choose_action(self, state, epsilon=True):\n",
        "        \"\"\"\n",
        "        Given a state `state`, return an action `(i, j)` to take.\n",
        "        If `epsilon` is `False`, then return the best action\n",
        "        available in the state (the one with the highest Q-value,\n",
        "        using 0 for pairs that have no Q-values).\n",
        "        If `epsilon` is `True`, then with probability\n",
        "        `self.epsilon` choose a random available action,\n",
        "        otherwise choose the best action available.\n",
        "        If multiple actions have the same Q-value, any of those\n",
        "        options is an acceptable return value.\n",
        "        \"\"\"\n",
        "\n",
        "        bestOpt = None\n",
        "        bestOutcome = 0\n",
        "        optionsAvail = list(Nim.available_actions(state))\n",
        "        for i in optionsAvail:\n",
        "            q_val = self.get_q_value(state, i)\n",
        "            if bestOpt is None or q_val > bestOutcome:\n",
        "                bestOutcome = q_val\n",
        "                bestOpt = i\n",
        "\n",
        "        if epsilon:\n",
        "            total_actions = len(optionsAvail)\n",
        "            weights = [(1 - self.epsilon) if i == bestOpt else self.epsilon for i in optionsAvail]\n",
        "            bestOpt = random.choices(optionsAvail, weights=weights, k=1)[0]\n",
        "\n",
        "        return bestOpt\n",
        "\n",
        "def train(n):\n",
        "    \"\"\"\n",
        "    Train an AI by playing `n` games against itself.\n",
        "    \"\"\"\n",
        "\n",
        "    player = NimAI()\n",
        "\n",
        "    # Play n games\n",
        "    for i in range(n):\n",
        "        print(f\"Playing training game {i + 1}\")\n",
        "        game = Nim()\n",
        "\n",
        "        # Keep track of last move made by either player\n",
        "        last = {\n",
        "            0: {\"state\": None, \"action\": None},\n",
        "            1: {\"state\": None, \"action\": None}\n",
        "        }\n",
        "\n",
        "        # Game loop\n",
        "        while True:\n",
        "\n",
        "            # Keep track of current state and action\n",
        "            state = game.piles.copy()\n",
        "            action = player.choose_action(game.piles)\n",
        "\n",
        "            # Keep track of last state and action\n",
        "            last[game.player][\"state\"] = state\n",
        "            last[game.player][\"action\"] = action\n",
        "\n",
        "            # Make move\n",
        "            game.move(action)\n",
        "            new_state = game.piles.copy()\n",
        "\n",
        "            # When game is over, update Q values with rewards\n",
        "            if game.winner is not None:\n",
        "                player.update(state, action, new_state, -1)\n",
        "                player.update(\n",
        "                    last[game.player][\"state\"],\n",
        "                    last[game.player][\"action\"],\n",
        "                    new_state,\n",
        "                    1\n",
        "                )\n",
        "                break\n",
        "\n",
        "            # If game is continuing, no rewards yet\n",
        "            elif last[game.player][\"state\"] is not None:\n",
        "                player.update(\n",
        "                    last[game.player][\"state\"],\n",
        "                    last[game.player][\"action\"],\n",
        "                    new_state,\n",
        "                    0\n",
        "                )\n",
        "\n",
        "    print(\"Done training\")\n",
        "\n",
        "    # Return the trained AI\n",
        "    return player\n",
        "\n",
        "\n",
        "def play(ai, human_player=None):\n",
        "    \"\"\"\n",
        "    Play human game against the AI.\n",
        "    `human_player` can be set to 0 or 1 to specify whether\n",
        "    human player moves first or second.\n",
        "    \"\"\"\n",
        "\n",
        "    # If no player order set, choose human's order randomly\n",
        "    if human_player is None:\n",
        "        human_player = random.randint(0, 1)\n",
        "\n",
        "    # Create new game\n",
        "    game = Nim()\n",
        "\n",
        "    # Game loop\n",
        "    while True:\n",
        "\n",
        "        # Print contents of piles\n",
        "        print()\n",
        "        print(\"Piles:\")\n",
        "        for i, pile in enumerate(game.piles):\n",
        "            print(f\"Pile {i}: {pile}\")\n",
        "        print()\n",
        "\n",
        "        # Compute available actions\n",
        "        available_actions = Nim.available_actions(game.piles)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # Let human make a move\n",
        "        if game.player == human_player:\n",
        "            print(\"Your Turn\")\n",
        "            while True:\n",
        "                pile = int(input(\"Choose Pile: \"))\n",
        "                count = int(input(\"Choose Count: \"))\n",
        "                if (pile, count) in available_actions:\n",
        "                    break\n",
        "                print(\"Invalid move, try again.\")\n",
        "\n",
        "        # Have AI make a move\n",
        "        else:\n",
        "            print(\"AI's Turn\")\n",
        "            pile, count = ai.choose_action(game.piles, epsilon=False)\n",
        "            print(f\"AI chose to take {count} from pile {pile}.\")\n",
        "\n",
        "        # Make move\n",
        "        game.move((pile, count))\n",
        "\n",
        "        # Check for winner\n",
        "        if game.winner is not None:\n",
        "            print()\n",
        "            print(\"GAME OVER\")\n",
        "            winner = \"Human\" if game.winner == human_player else \"AI\"\n",
        "            print(f\"Winner is {winner}\")\n",
        "            return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGbq8sjSvy_C"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}